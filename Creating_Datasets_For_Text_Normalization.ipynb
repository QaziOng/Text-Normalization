{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/P/LdPjIvbyvByM9slb98"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ðŸš€ Text Normalization Dataset Generator with Expanded Sentences\n","import pandas as pd\n","import random\n","from google.colab import files\n","\n","# Expanded base sentences (now ~80+ examples)\n","base_sentences = [\n","    # Greetings & small talk\n","    \"hello how are you doing today\",\n","    \"good morning have a nice day\",\n","    \"good night sweet dreams\",\n","    \"hi there long time no see\",\n","    \"see you soon my friend\",\n","    \"thanks a lot for your help\",\n","    \"happy birthday to you\",\n","    \"congratulations on your success\",\n","    \"have a safe journey\",\n","    \"take care and stay safe\",\n","\n","    # Casual conversation\n","    \"what are you doing right now\",\n","    \"i will meet you at the park tomorrow\",\n","    \"let us go for a walk\",\n","    \"cannot wait for the weekend\",\n","    \"i am so happy and excited\",\n","    \"weather is great today\",\n","    \"this movie was fantastic\",\n","    \"the exam was really hard\",\n","    \"python is my favorite language\",\n","    \"i love natural language processing\",\n","\n","    # Work/study related\n","    \"please send me the report asap\",\n","    \"i need to finish my homework\",\n","    \"the deadline is next monday\",\n","    \"let us schedule a meeting for tomorrow\",\n","    \"i will call you after the class\",\n","    \"project submission is due tonight\",\n","    \"machine learning is awesome\",\n","    \"data science requires a lot of practice\",\n","    \"artificial intelligence is the future\",\n","    \"text normalization is very important\",\n","\n","    # Questions & answers\n","    \"where are you going now\",\n","    \"do you know the answer\",\n","    \"can you please help me\",\n","    \"what time is the train\",\n","    \"is it going to rain today\",\n","    \"are you free this evening\",\n","    \"when will you be back\",\n","    \"did you complete the task\",\n","    \"who is your best friend\",\n","    \"why are you so late\",\n","\n","    # Social media/chat slang\n","    \"this song is lit\",\n","    \"that was a savage reply\",\n","    \"lol that is so funny\",\n","    \"brb need to grab food\",\n","    \"idk what to say\",\n","    \"btw did you check instagram\",\n","    \"omg that is crazy\",\n","    \"smh i cannot believe it\",\n","    \"yolo lets do this\",\n","    \"fyi the meeting is cancelled\",\n","\n","    # News/event style\n","    \"the president will speak tonight\",\n","    \"the football match starts at 9 pm\",\n","    \"the new iphone was just announced\",\n","    \"the economy is growing faster this year\",\n","    \"the weather forecast predicts heavy rain\",\n","    \"scientists discovered a new species\",\n","    \"the concert tickets are sold out\",\n","    \"traffic is heavy on the main road\",\n","    \"the movie will be released next friday\",\n","    \"the festival begins tomorrow\",\n","\n","    # Mixed pronouns & tenses\n","    \"she is reading a book\",\n","    \"he went to the market yesterday\",\n","    \"they are playing football outside\",\n","    \"we will travel to paris next month\",\n","    \"i had breakfast already\",\n","    \"you should drink more water\",\n","    \"i was watching tv when you called\",\n","    \"he is working from home today\",\n","    \"they will arrive in an hour\",\n","    \"we are learning python programming\",\n","]\n","\n","# Variations to mess up text (added a few more)\n","variations = [\n","    lambda s: s.upper(),\n","    lambda s: s.capitalize(),\n","    lambda s: s.replace(\"you\", \"u\"),\n","    lambda s: s.replace(\"are\", \"r\"),\n","    lambda s: s + \"!!!\",\n","    lambda s: \"   \" + s + \"   \",\n","    lambda s: s.replace(\"today\", \"2day\"),\n","    lambda s: s.replace(\"to\", \"2\"),\n","    lambda s: s.replace(\"and\", \"&\"),\n","    lambda s: s.replace(\"asap\", \"ASAP\"),\n","    lambda s: s.replace(\"great\", \"gr8\"),\n","    lambda s: s.replace(\"excited\", \"excitedddd\"),\n","    lambda s: s.replace(\"cannot\", \"can't\"),\n","    lambda s: s.replace(\"thanks\", \"thx\"),\n","    lambda s: s.replace(\"morning\", \"mornin\"),\n","    lambda s: s.replace(\"for\", \"4\"),\n","    lambda s: s.replace(\"with\", \"w/\"),\n","    lambda s: s.replace(\"because\", \"cuz\"),\n","    lambda s: s.replace(\"people\", \"ppl\"),\n","    lambda s: s.replace(\"before\", \"b4\"),\n","    lambda s: s.replace(\"please\", \"plz\"),\n","    lambda s: s.replace(\"love\", \"luv\"),\n","    lambda s: s.replace(\"very\", \"vry\"),\n","    lambda s: s.replace(\"okay\", \"ok\"),\n","    lambda s: s.replace(\"message\", \"msg\"),\n","    lambda s: s.replace(\"tomorrow\", \"tmrw\"),\n","    lambda s: s + random.choice([\" ðŸ˜‚\", \" ðŸ˜Ž\", \" ðŸ¤”\", \" ðŸ˜­\"]),\n","    lambda s: ''.join(random.choice([c, c+c]) for c in s),  # extra letters\n","]\n","\n","# Function to mess up text randomly\n","def mess_up(sentence):\n","    s = sentence\n","    for _ in range(random.randint(1, 3)):\n","        func = random.choice(variations)\n","        s = func(s)\n","    return s\n","\n","# Function to generate dataset and download\n","def generate_dataset(num_rows, file_name):\n","    rows = []\n","    for _ in range(num_rows):\n","        sent = random.choice(base_sentences)\n","        messy = mess_up(sent)\n","        rows.append({\"raw_text\": messy, \"expected_normalized_text\": sent})\n","    df = pd.DataFrame(rows)\n","    df.to_csv(file_name, index=False)\n","    print(f\"âœ… Dataset saved as {file_name} with {num_rows} rows.\")\n","    files.download(file_name)\n","\n","# Generate datasets\n","#generate_dataset(50000, \"text_normalization_dataset_small.csv\")\n","generate_dataset(200000, \"text_normalization_dataset_large.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"ULikSA8-eD1x","executionInfo":{"status":"ok","timestamp":1756971338361,"user_tz":-300,"elapsed":1781,"user":{"displayName":"Revolution Beast","userId":"01829344225461195999"}},"outputId":"9fac86fd-51be-40c9-96e0-556352dec565"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Dataset saved as text_normalization_dataset_large.csv with 200000 rows.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_32c8a004-4f40-4949-929c-c0f4b65a240f\", \"text_normalization_dataset_large.csv\", 11709057)"]},"metadata":{}}]}]}